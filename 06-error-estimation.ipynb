{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>CS4619: Artificial Intelligence 2</h1>\n",
    "<h2>Error Estimation</h2>\n",
    "<h3>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Initialization $\\newcommand{\\Set}[1]{\\{#1\\}}$ $\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ $\\newcommand{\\v}[1]{\\pmb{#1}}$ $\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ $\\newcommand{\\rv}[1]{[#1]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import LeaveOneOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Mean Squared Error</h1>\n",
    "<p>\n",
    "    So, you've trained an estimator on a training set. \n",
    "    You want to know how well it will do in practice, once you start to use it to make predictions.\n",
    "    Easy right? We have the training set, so we measure how well the estimator peforms on the training set.\n",
    "    For each example in the training set, we ask the estimator to predict the value of the dependent variable\n",
    "    and compare with the <em>actual</em> value, which is also in the training set.\n",
    "</p>\n",
    "<p>\n",
    "    For regression, we will compute the <b>mean squared error</b>:\n",
    "    $$\\frac{1}{m}\\sum_{i=0}^m(\\hat{y_i} - y_i)^2$$\n",
    "    where $\\hat{y_i}$ is the predicted value for example $i$ and $y_i$ is the actual value.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Example of Mean Squared Error on the Training Set</h2>\n",
    "<p>\n",
    "    Let's compute the mean squared error for Linear Regression trained on the Cork property dataset: \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55456.553237981032"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use pandas to read the CSV file\n",
    "df = pd.read_csv(\"dataset-corkA.csv\")\n",
    "\n",
    "# Get the feature-values and the target values into separate numpy arrays of numbers\n",
    "X = df[['flarea', 'bdrms', 'bthrms']].values\n",
    "y = df['price'].values\n",
    "\n",
    "# Create linear regression object\n",
    "estimator = LinearRegression()\n",
    "\n",
    "# Train the model using the data\n",
    "estimator.fit(X, y)\n",
    "\n",
    "# Use the learned model to predict on the same examples\n",
    "y_predicted = estimator.predict(X)\n",
    "\n",
    "# Compute the mean squared error\n",
    "mse = mean_squared_error(y, y_predicted)\n",
    "\n",
    "# Display\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Training Error and Test Error</h1>\n",
    "<p>\n",
    "    We will refer to the error on the training set as the <b>training error</b>. (Some people call it the\n",
    "    'resubstitution error' and sometimes the 'in-sample error'.) But, remember, we're not much\n",
    "    interested in how well we have done on this data; we want to know how well we will perform in the future, \n",
    "    on unseen data. Is the training error a good indicator of performance on unseen data? The answer is, \n",
    "    in general: no.\n",
    "</p>\n",
    "<p>\n",
    "    The estimator's training error (its performance on the very data on which it was trained) is likely to\n",
    "    give an optimistic, even very optimistic, view of its future performance.\n",
    "</p>\n",
    "<ul>\n",
    "    <li>\n",
    "        One intuition of why this is wrong is that it's a bit like\n",
    "        a teacher who sets exams whose questions test the very same examples s/he used when teaching the\n",
    "        material.\n",
    "    </li>\n",
    "    <li>\n",
    "        Another intuition of why this is wrong: which estimator that we have studied can have\n",
    "        zero training error but would be likely to perform much less well in practice?\n",
    "    </li>\n",
    "</ul>\n",
    "<p>\n",
    "    (By the way, although the training error is not a good predictor of future performance, it can \n",
    "    still be useful, as we will see in the lecture on Underfitting and Overfitting.)\n",
    "</p>\n",
    "<p>\n",
    "    To predict future performance, we need to measure error on an <em>independent</em> dataset &mdash; one\n",
    "    that played no part in creating the estimator. This second dataset is called the <b>test set</b>, and\n",
    "    our error on the test set we will call the <b>test error</b>. (In some circumstances people might call it \n",
    "    the 'out-of-sample error' or 'extra-sample error'.)\n",
    "</p>\n",
    "<ul>\n",
    "    <li>\n",
    "        If you have a ready supply of quality data, then collect one very large dataset \n",
    "        to be the training\n",
    "        set, and collect another very large dataset to be the test set. But large datasets are not\n",
    "        always available, and large high-quality datasets are even harder to come by. (And, remember, it must also \n",
    "        have the actual values of the dependent variable as well as the values of the features.)\n",
    "    </li>\n",
    "    <li>\n",
    "        If the supply of data is more limited, then collect one dataset (as large as you can) and partition it \n",
    "        into training set and test set. \n",
    "        This is called the <b>holdout</b> method, because the test set\n",
    "        is withheld during training. It is essential that the test set is not used in any way to create\n",
    "        the estimator. We look at holdout, and variations of it, in more detail in this lecture.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Holdout</h1>\n",
    "<p>\n",
    "   We split the dataset randomly but ensuring the two sets are <em>disjoint</em>.\n",
    "   There is a tension here. To learn a good estimator, we want the training set to be as big as possible.\n",
    "   But for a good prediction of future performance, we want the test set to be as big as possible.\n",
    "   Commonly, the training set will be between 50% and 80% of the dataset.\n",
    "</p>\n",
    "<p>\n",
    "    Splitting the dataset in this way is very easy in scikit-learn:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8848.4480660590198, 241423.1846134419)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "# Create linear regression object\n",
    "estimator = LinearRegression()\n",
    "\n",
    "# Train the model using the training set\n",
    "estimator.fit(X_train, y_train)\n",
    "\n",
    "# The training error\n",
    "# Predict on the training set and measure the difference between the predictions and the actual values in the training set:\n",
    "y_predicted = estimator.predict(X_train)\n",
    "mse_train = mean_squared_error(y_train, y_predicted)\n",
    "\n",
    "# The test error\n",
    "# Predict on the test set and measure the difference between the predictions and the actual values in the test set:\n",
    "y_predicted = estimator.predict(X_test)\n",
    "mse_test = mean_squared_error(y_test, y_predicted)\n",
    "\n",
    "# Display\n",
    "mse_train, mse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    You will find it instructive to run the above again and again to see the effect of different random splits.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Pros and Cons of Holdout</h2>\n",
    "<p>\n",
    "    The advantage of this method is that the test error is independent of the training set.\n",
    "</p>\n",
    "<p>\n",
    "    The disadvantages of the holdout method are:\n",
    "</p>\n",
    "<ul>\n",
    "    <li>\n",
    "        You will observe that results can vary quite a lot. Informally, you might get lucky &mdash; or unlucky.\n",
    "        Maybe you get a very 'helpful' training set, or a very 'unhelpful' training set; a very 'easy-to-predict' \n",
    "        test set, or a very 'hard-to-predict' test set. In other words,\n",
    "        in any one split, the data used for training or testing might not be representative.\n",
    "    </li>\n",
    "    <li>\n",
    "        We are training on only a subset of the available dataset, perhaps as little as 50% of it. From so little\n",
    "        data, we may learn a worse model and so our error measurement may be pessimistic.\n",
    "    </li>\n",
    "</ul>\n",
    "<p>\n",
    "    In practice, you would not use the holdout method &mdash; unless you had a very large dataset that\n",
    "    would mitigate the above problems. Instead, you would use one of its variants that we \n",
    "    describe below. Each of these variants uses <b>resampling</b>, meaning that the examples get re-used\n",
    "    for training and testing.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Repeated Holdout</h1>\n",
    "<p>\n",
    "    One solution to the problem of biased holdout sets is to <em>repeat</em> the whole process:\n",
    "</p>\n",
    "<ul style=\"background: lightgray; list-style: none\">\n",
    "    <li>\n",
    "        repeatedly\n",
    "        <ul>\n",
    "            <li>split the dataset into training and test sets</li>\n",
    "            <li>train on the training set</li>\n",
    "            <li>make predictions for the test set</li>\n",
    "            <li>measure error (e.g. MSE)</li>\n",
    "        </ul>\n",
    "        report the mean of the errors\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Illustrating scikit-learn's ShuffleSplit Class</h2>\n",
    "<p>\n",
    "    scikit-learn provides a ShuffleSplit class, which gives Boolean indexes that split the dataset. Here's a simple use:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [119  76 203  21 162  78 131 128 199 193 180 188  89 192  87 117   0 140\n",
      " 150  24 183  72  26  31  88   5 191 172  51 220 160  38 186 165  70 102\n",
      "   1  11 185 133 178  43 104 138 118 149  61 121 115 214 222  97  99 200\n",
      " 107  69  98 209  82  47  93 223 129 111  37  96  19 143  75 212 132  58\n",
      " 106  35 201  22  57  84  62  85  12 171  27 116  48  81 105 136 145  44\n",
      "  36 147 112  32 114  66  68  20 164 218 219  90  63 148 130 182 152 206\n",
      " 163 213 151  64  83  33 216 196  42 103  77   3 167 101  80 108  39 139\n",
      "  91 170  16   9  14   2 137 190 208  49  46 154  95 110 158 155  53 157\n",
      " 141  41 126 174  73 194 142  71  28 122   8   6]\n",
      "TEST: [134 179  92 211   4  67  17  25 173  79 153 124  65 210 161  56  54  55\n",
      "  29 207  18 156 100 166 123  52  15  13 195 175  59 159 113 198 135  34\n",
      "  60  50   7 125 197 202 168  10 127  45 215 184 146 204  86  40 177 221\n",
      "  94 169  23  74 189 109 120 176 205  30 217 187 144 181]\n",
      "TRAIN: [ 48  11  62   6  19  92 131  37 124 138 179  75 212 188  56 222  49 110\n",
      " 148  97  51  33  94  22  50 133 194 104  24  81  95  43  20  47  64  21\n",
      "  31 186 189 160  26 101  38  79  27 206 123 202  73 178 174  16  30 109\n",
      "  90 180 168  17  23 102 114 118  86 223   8 157  69   9 125 152 207  45\n",
      " 158 176   2 144 140  68  77  41  88 195  53  13 170 164 214 156 218 113\n",
      "  93 121 163 171  58 143 200 149  76 130  67 213 210 173 220 107 215  42\n",
      " 115 161 183   7 116 191  72 211 198 154 147  74  87 127  70 204  59  61\n",
      "  36  35 184 141  84 103 216  82  32  78  85 111 197  83   0 193  99 117\n",
      "  28  39 145 137 139 153  25 172  40 201  29  15]\n",
      "TEST: [ 10 175 105 134 167  18 181 142  71 120  57 219   1  46 205 128  98 187\n",
      "   5 151  34 135 221 190 106 208  12 196  55 150 129  65  96 122 159  63\n",
      "   3  66 155 169 217 146  91   4  14 177 182 162 119 209  60 126 108 165\n",
      "  52 192 203  89  44 112 185  54 166 100 199 132  80 136]\n",
      "TRAIN: [211 115  75 149 199  45  16  49 186 210  92  74 172 189 164  68 145 148\n",
      "  79  42  29  63  97 125 202  10 137 118  64 159  44 206 114 101 194  58\n",
      " 214 178  57  51  76  20  98 102 158 205  47 221  99 123 183 122  59  14\n",
      " 105 132 146 169 160 187 135   6  85 113 212   0  27  55 175  18 203  48\n",
      " 193  62  36  69 139  43  60  95 109 150  24 140  81   1 126  56 119 163\n",
      "  13  23  86 171  78 128  52 181  34 192 141 180 177 176  89  84 174   2\n",
      " 143 134  11 142  35  82 223 108 127 191 151 104 216 157  15 130  87  94\n",
      "  37  77   3 117 162  80 147   4 204   7  22 152 131 100  30  65  61  32\n",
      " 153 155  21 136 195 165 179 220 156  67 116 196]\n",
      "TEST: [217 166 182 110 138 201 184  53 219  38 208 111 107 190 121  91  19  31\n",
      "  41   9 154 168 218 173 129 207  88 222  33 161 112  25  96 209  72  71\n",
      "  54 200  83 198  66 124  17  39 133 170  73  12 185 215 167  26  46  28\n",
      " 188 213   8 103 106 197 120 144  90  50   5  40  93  70]\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset 70%/30%. Do so 3 times.\n",
    "ss = ShuffleSplit(n_splits = 3, test_size = 0.3)\n",
    "\n",
    "# Display the indexes\n",
    "for train_indexes, test_indexes in ss.split(X):\n",
    "    print(\"TRAIN:\", train_indexes)\n",
    "    print(\"TEST:\", test_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using ShuffleSplit to Compute Training Error and Test Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48649.124348839061, 85196.270987307231)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def repeated_holdout_for_regression(estimator, X, y, n_splits = 10, test_size = 0.3):\n",
    "    mses_train = np.zeros(n_splits)\n",
    "    mses_test = np.zeros(n_splits)\n",
    "    ss = ShuffleSplit(n_splits, test_size = test_size)\n",
    "    for i, (train_indexes, test_indexes) in zip(range(n_splits), ss.split(X)):\n",
    "        X_train = X[train_indexes]\n",
    "        y_train = y[train_indexes]\n",
    "        X_test = X[test_indexes]\n",
    "        y_test = y[test_indexes]\n",
    "        estimator.fit(X_train, y_train)\n",
    "        y_predicted = estimator.predict(X_train)\n",
    "        mses_train[i] = mean_squared_error(y_train, y_predicted)\n",
    "        y_predicted = estimator.predict(X_test)\n",
    "        mses_test[i] = mean_squared_error(y_test, y_predicted)\n",
    "    return np.mean(mses_train), np.mean(mses_test)\n",
    "\n",
    "# Here's an example of calling the function:\n",
    "estimator = LinearRegression()\n",
    "mean_mse_train, mean_mse_test = repeated_holdout_for_regression(estimator, X, y)\n",
    "mean_mse_train, mean_mse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Using ShuffleSplit to Compute Test Error</h2>\n",
    "<p>\n",
    "    scikit-learn does provide a more convenient way of doing this, but it only computes the test error and it computes the negative of the MSE (so that higher values are better):\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-68390.020459469481"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = ShuffleSplit(n_splits = 10, test_size = 0.3)\n",
    "estimator = LinearRegression()\n",
    "mses_test = cross_val_score(estimator, X, y, scoring = 'neg_mean_squared_error', cv = ss)\n",
    "mean_mse_test = np.mean(mses_test)\n",
    "mean_mse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Pros and Cons of Repeated Holdout</h2>\n",
    "<p>\n",
    "    The advantage here is we can repeat indefinitely to improve our confidence. The disadvantage is training\n",
    "    sets may overlap with each other and test sets may overlap with each other, although the effect of this\n",
    "    is reduced if the dataset is large.\n",
    "</p>\n",
    "<p>\n",
    "    Let's look at another method.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>$k$-Fold Cross-Validation</h1>\n",
    "<p>\n",
    "    In this approach, we randomly partition the data into $k$ disjoint subsets of equal size. (This is a different use of \n",
    "    $k$ from the $k$ in kNN.) Each of the partitions is called a <b>fold</b>. Typically, $k = 10$, so you have 10 folds. \n",
    "    But, for conventional statistical significance testing to be applicable, you should probably ensure that the number of\n",
    "    examples in each fold does not fall below 30. If this isn't possible, then either use a smaller value for $k$, or\n",
    "    do not use $k$-fold cross validation. \n",
    "</p>\n",
    "<p>\n",
    "    You take each fold in turn and use it as the test set, training the learner on \n",
    "    the remaining folds. Clearly, you can do this $k$ times, so that each fold gets 'a turn' at being the test set.\n",
    "</p>\n",
    "<ul style=\"background: lightgray; list-style: none\">\n",
    "    <li>\n",
    "        partition the dataset $D$ into $k$ disjoint equal-sized subsets, $T_1, T_2,\\ldots,T_k$\n",
    "    <li>\n",
    "    <li>\n",
    "        <b>for</b> $i = 1$ to $k$\n",
    "        <ul>\n",
    "            <li>train on $D \\setminus T_i$</li>\n",
    "            <li>make predictions for $T_i$</li>\n",
    "            <li>measure error (e.g. MSE)</li>\n",
    "        </ul>\n",
    "        report the mean of the errors\n",
    "    </li>\n",
    "</ul>\n",
    "<p>\n",
    "    By this method, each example is used exactly once for testing, and $k - 1$ times for training.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Pros and Cons of $k$-Fold Cross-Validation</h2>\n",
    "<p>\n",
    "    Compared with repeated holdout, the advantages of this method are:\n",
    "</p>\n",
    "<ul>\n",
    "    <li>\n",
    "        The test errors of the folds are independent &mdash; because examples are included in only one test set. \n",
    "    </li>\n",
    "    <li>\n",
    "        Better use is made of the dataset: for $k = 10$, for example, we train using 9/10 of the dataset.\n",
    "    </li>\n",
    "</ul>\n",
    "<p>\n",
    "     The disadvantages are: \n",
    "</p>\n",
    "<ul>\n",
    "    <li>\n",
    "        While the test sets are independent of each other, the training sets are not: they will overlap\n",
    "        with each other to some degree. (This effect of this will be less, of course, for larger datasets.)\n",
    "    </li>\n",
    "    <li>\n",
    "        The number of folds is constrained by the size of the dataset and the desire to have folds of\n",
    "        at least 30 examples.\n",
    "    </li>\n",
    "    <li>\n",
    "        It can be costly to train the learning algorithm $k$ times.\n",
    "    </li>\n",
    "    <li>\n",
    "        There may still be some variability in the results due to 'lucky'/'unlucky' splits &mdash; which\n",
    "        motivates Repeated $k$-Fold Cross-Validation, below.\n",
    "    </li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Illustrating scikit-learn's KFold Class</h2>\n",
    "<p>\n",
    "    scikit-learn provides the KFold class, which is an iterator, similar to the ShuffleSplit class. Here's a simple use:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [  0   1   2   3   4   6   7   8   9  11  12  14  15  16  18  21  22  23\n",
      "  24  25  26  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42\n",
      "  44  46  47  48  49  50  51  52  53  54  55  56  58  59  61  62  64  66\n",
      "  67  68  69  70  72  74  75  77  79  80  81  82  84  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  99 100 101 102 103 104 106 107 108 109 110\n",
      " 111 112 113 116 118 119 120 122 123 124 125 127 128 129 131 132 133 135\n",
      " 136 137 138 139 142 143 144 145 146 147 148 149 150 151 152 153 154 155\n",
      " 156 158 159 160 161 164 165 166 167 168 169 170 171 172 173 174 175 177\n",
      " 178 179 180 181 182 183 184 186 187 188 189 190 192 196 197 198 199 200\n",
      " 201 202 203 204 205 206 207 208 209 210 211 212 213 215 216 218 220]\n",
      "TEST: [  5  10  13  17  19  20  27  43  45  57  60  63  65  71  73  76  78  83\n",
      "  85  98 105 114 115 117 121 126 130 134 140 141 157 162 163 176 185 191\n",
      " 193 194 195 214 217 219 221 222 223]\n",
      "TRAIN: [  1   3   4   5   6   7   8   9  10  11  12  13  15  16  17  18  19  20\n",
      "  22  24  25  26  27  28  29  30  31  32  33  34  35  37  38  39  40  41\n",
      "  42  43  44  45  49  50  51  52  53  54  56  57  58  60  61  63  64  65\n",
      "  69  70  71  72  73  75  76  77  78  79  80  83  84  85  86  87  88  89\n",
      "  90  91  94  95  98  99 100 101 102 103 105 107 108 110 112 113 114 115\n",
      " 116 117 118 119 121 122 123 124 125 126 128 129 130 131 132 134 137 138\n",
      " 139 140 141 142 143 144 146 147 148 149 150 151 157 158 159 160 161 162\n",
      " 163 165 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182\n",
      " 183 184 185 186 187 189 191 192 193 194 195 197 199 200 201 202 203 204\n",
      " 205 206 208 209 210 211 212 214 215 216 217 218 219 220 221 222 223]\n",
      "TEST: [  0   2  14  21  23  36  46  47  48  55  59  62  66  67  68  74  81  82\n",
      "  92  93  96  97 104 106 109 111 120 127 133 135 136 145 152 153 154 155\n",
      " 156 164 166 188 190 196 198 207 213]\n",
      "TRAIN: [  0   2   3   4   5   8  10  11  12  13  14  15  17  19  20  21  23  25\n",
      "  27  28  29  30  31  32  33  34  35  36  37  39  40  41  42  43  44  45\n",
      "  46  47  48  49  50  51  52  53  54  55  57  58  59  60  61  62  63  65\n",
      "  66  67  68  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85\n",
      "  87  90  92  93  95  96  97  98 100 102 104 105 106 108 109 110 111 112\n",
      " 113 114 115 116 117 120 121 122 123 124 125 126 127 128 130 131 132 133\n",
      " 134 135 136 137 138 139 140 141 143 145 146 147 148 149 150 152 153 154\n",
      " 155 156 157 158 160 161 162 163 164 165 166 167 169 170 173 175 176 179\n",
      " 181 182 183 184 185 186 188 189 190 191 193 194 195 196 198 200 201 202\n",
      " 205 206 207 208 209 212 213 214 215 216 217 218 219 220 221 222 223]\n",
      "TEST: [  1   6   7   9  16  18  22  24  26  38  56  64  69  70  86  88  89  91\n",
      "  94  99 101 103 107 118 119 129 142 144 151 159 168 171 172 174 177 178\n",
      " 180 187 192 197 199 203 204 210 211]\n",
      "TRAIN: [  0   1   2   3   5   6   7   8   9  10  11  12  13  14  16  17  18  19\n",
      "  20  21  22  23  24  26  27  29  30  33  35  36  38  40  43  44  45  46\n",
      "  47  48  49  51  54  55  56  57  58  59  60  61  62  63  64  65  66  67\n",
      "  68  69  70  71  72  73  74  75  76  77  78  81  82  83  85  86  87  88\n",
      "  89  90  91  92  93  94  96  97  98  99 100 101 103 104 105 106 107 108\n",
      " 109 110 111 113 114 115 116 117 118 119 120 121 126 127 128 129 130 131\n",
      " 132 133 134 135 136 138 139 140 141 142 144 145 147 151 152 153 154 155\n",
      " 156 157 159 161 162 163 164 166 168 171 172 173 174 176 177 178 179 180\n",
      " 182 185 187 188 189 190 191 192 193 194 195 196 197 198 199 200 202 203\n",
      " 204 206 207 208 209 210 211 212 213 214 215 216 217 219 221 222 223]\n",
      "TEST: [  4  15  25  28  31  32  34  37  39  41  42  50  52  53  79  80  84  95\n",
      " 102 112 122 123 124 125 137 143 146 148 149 150 158 160 165 167 169 170\n",
      " 175 181 183 184 186 201 205 218 220]\n",
      "TRAIN: [  0   1   2   4   5   6   7   9  10  13  14  15  16  17  18  19  20  21\n",
      "  22  23  24  25  26  27  28  31  32  34  36  37  38  39  41  42  43  45\n",
      "  46  47  48  50  52  53  55  56  57  59  60  62  63  64  65  66  67  68\n",
      "  69  70  71  73  74  76  78  79  80  81  82  83  84  85  86  88  89  91\n",
      "  92  93  94  95  96  97  98  99 101 102 103 104 105 106 107 109 111 112\n",
      " 114 115 117 118 119 120 121 122 123 124 125 126 127 129 130 133 134 135\n",
      " 136 137 140 141 142 143 144 145 146 148 149 150 151 152 153 154 155 156\n",
      " 157 158 159 160 162 163 164 165 166 167 168 169 170 171 172 174 175 176\n",
      " 177 178 180 181 183 184 185 186 187 188 190 191 192 193 194 195 196 197\n",
      " 198 199 201 203 204 205 207 210 211 213 214 217 218 219 220 221 222 223]\n",
      "TEST: [  3   8  11  12  29  30  33  35  40  44  49  51  54  58  61  72  75  77\n",
      "  87  90 100 108 110 113 116 128 131 132 138 139 147 161 173 179 182 189\n",
      " 200 202 206 208 209 212 215 216]\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "# Display the indexes\n",
    "for train_indexes, test_indexes in kf.split(X):\n",
    "    print(\"TRAIN:\", train_indexes)\n",
    "    print(\"TEST:\", test_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Using KFold to Compute Test Error</h2>\n",
    "<p>\n",
    "    Assuming that we are happy to get just the test error, we can use the cross_val_score method again:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-71316.054085133728"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = KFold(n_splits =10, shuffle = True)\n",
    "estimator = LinearRegression()\n",
    "mses_test = cross_val_score(estimator, X, y, scoring = 'neg_mean_squared_error', cv = kf)\n",
    "mean_mse_test = np.mean(mses_test)\n",
    "mean_mse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    But, $k$-fold cross-validation is so commonplace, that there is a shorter way to write\n",
    "    the code above, as follows:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-100047.69646341191"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = LinearRegression()\n",
    "mses_test = cross_val_score(estimator, X, y, scoring = 'neg_mean_squared_error', cv = 10)\n",
    "mean_mse_test = np.mean(mses_test)\n",
    "mean_mse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Be warned, however, this almost certainly does not shuffle the dataset before splitting it into folds.\n",
    "    Q: Why might that be a problem?\n",
    "</p>\n",
    "<p>\n",
    "    You should probably shuffle the <code>DataFrame</code> just after reading it in from the CSV file using, e.g.:<br>\n",
    "    <code>df = df.take(np.random.permutation(len(df)))</code>\n",
    "</p>\n",
    "<p>\n",
    "    Final observation: In the above, we ran the 10-fold cross validation on the Cork property dataset. That dataset has \n",
    "    only 224 examples\n",
    "    &mdash; not enough examples to give at least 30 examples in each of the 10 folds. So this isn't an ideal use of \n",
    "    the method.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Repeated $k$-Fold Cross-Validation</h1>\n",
    "<p>\n",
    "    It's not uncommon to find people repeating the $k$-fold cross validation to reduce variability in the results. \n",
    "    For example, you might run 10 times 10-fold\n",
    "    cross-validation and average the results. This means running the learning algorithm 100 times, each time on a training\n",
    "    set that is nine tenths of the full dataset &mdash; quite computationally expensive.\n",
    "</p>\n",
    "<p>\n",
    "    We won't look at the code. Straightforwardly, you wrap an extra loop around the code we gave above. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Leave-One-Out Cross-Validation (LOOCV)</h1>\n",
    "<p>\n",
    "    Leave-one-out cross-validation is $k$-fold cross-validation in which $k = m$, the number of examples in the dataset:\n",
    "    each example is in its own fold. In other words, you train the learner on all examples but one, and that one remaining\n",
    "    example is used for testing. And you do this in turn for each example in the dataset. You'll get $m$ error values, which you\n",
    "    can average.\n",
    "</p>\n",
    "<ul style=\"background: lightgray; list-style: none\">\n",
    "    <li>\n",
    "        <b>for</b> $i = 1$ to $m$\n",
    "        <ul>\n",
    "            <li>train on $D \\setminus \\Set{\\v{x}^{(i)}}$</li>\n",
    "            <li>make prediction for $\\v{x}^{(i)}$</li>\n",
    "            <li>measure error (e.g. MSE)</li>\n",
    "        </ul>\n",
    "        report the mean of the errors\n",
    "    </li>\n",
    "</ul>\n",
    "<p>\n",
    "    As with $k$-fold cross-validation, each example is used exactly once in a test set. But each example is used in $m - 1$ \n",
    "    different training sets. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Pros and Cons of LOOCV</h2>\n",
    "<p>\n",
    "    There are advantages:\n",
    "</p>\n",
    "<ul>\n",
    "    <li>\n",
    "        One advantage of LOOCV is that the maximum amount of data is used for training, which makes an accurate\n",
    "        estimator more likely. (But, see the disadvantage below.)\n",
    "    </li>\n",
    "    <li>\n",
    "        Another advantage is that there is no randomness: we can't get lukcy or unlucky. And there's \n",
    "        no point in repeating the process, we'll get the same result each time.\n",
    "    </li>\n",
    "</ul>\n",
    "<p>\n",
    "    But there are disadvantages:\n",
    "</p>\n",
    "<ul>\n",
    "    <li>\n",
    "        The obvious disadvantage is the cost: the learner must be trained $m$ times, and each time it will be trained on almost \n",
    "        all the data. This method is therefore infeasible in some cases. \n",
    "        <p>\n",
    "            Question:\n",
    "        </p>\n",
    "        <ul>\n",
    "            <li>\n",
    "                For which estimator do you think LOOCV is fairly common? Why?\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        More subtly, LOOCV's $m$ models are trained on almost identical data; in $k$-Fold Cross-Validation, the $k$ models\n",
    "        are trained on data with less overlap.\n",
    "    </li>\n",
    "</ul>\n",
    "<p>\n",
    "    (Advanced note, which you can ignore: We said that LOOCV must train the learning algorithm $m$ times. In fact, \n",
    "    for some learners, including OLS linear regression, you can learn just the first model and then, with a bit of \n",
    "    maths, work out the final average error without learning any of the other models. So this makes LOOCV practical \n",
    "    for this class of learning algorithms.)\n",
    "</p>\n",
    "<p>\n",
    "    You can see that there is a trade-off here. Empirically, $k$-Fold Cross-Validation with $k = 5$ or $k = 10$ tends\n",
    "    to report the most reliable error figures.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Using LeaveOneOut to Compute Test Error</h2>\n",
    "<p>\n",
    "    Here is LOOCV in scikit-learn:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-72376.115691746367"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loocv = LeaveOneOut()\n",
    "estimator = LinearRegression()\n",
    "mses_test = cross_val_score(estimator, X, y, scoring = 'neg_mean_squared_error', cv = loocv)\n",
    "mean_mse_test = np.mean(mses_test)\n",
    "mean_mse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Final Remarks</h1>\n",
    "<ol>\n",
    "    <li>\n",
    "        There are methods other than those covered including Bootstrapping and Permutation Tests.\n",
    "    </li>\n",
    "    <li>\n",
    "        So you've used one of the above methods and found the test error of your estimator. The dirty secret\n",
    "        of Machine Learning is this: at this point, if dissatisfied with the test error, many Machine\n",
    "        Learning researchers, start tweaking their learning algorithms to try to bring down the test error.\n",
    "        This is wrong! It is called <b>leakage</b>: knowledge of the test set is being used to develop the\n",
    "        estimator. It's like the teacher letting the students take the same exam again. It will result in\n",
    "        the test error giving an optimistic view of the ultimate performance of the estimator on unseen data.\n",
    "        <p>\n",
    "            If you must do something like this, then somewhat less problematic is if you ensure that you are \n",
    "            using different random splits when evaluating your tweaks.\n",
    "        </p>\n",
    "    </li>\n",
    "    <li>\n",
    "        Finally, suppose you have used one of the above methods to estimate the error of your regressor. \n",
    "        You are ready to release your regressor on the world. At this point, you can train it on\n",
    "        <em>all</em> the examples in your dataset, so as to maximize the use of the data.\n",
    "    </li>\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
