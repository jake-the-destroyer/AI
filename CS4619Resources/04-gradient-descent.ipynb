{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>CS4619: Artificial Intelligence 2</h1>\n",
    "<h2>OLS Linear Regression using Gradient Descent</h2>\n",
    "<h3>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Initialization $\\newcommand{\\Set}[1]{\\{#1\\}}$ $\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ $\\newcommand{\\v}[1]{\\pmb{#1}}$ $\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ $\\newcommand{\\rv}[1]{[#1]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>OLS Regression using Gradient Descent</h1>\n",
    "<p>\n",
    "    A reminder: we have a training set containing $m$ examples, each having $n$ features. We have \n",
    "    represented the training set as an $m \\times(n+1)$ matrix $\\v{X}$: each row is one of the examples;\n",
    "    each column is one of the features; but all the values in the first column (which we designate column 0)\n",
    "    are set to 1. Each hypothesis is a linear equation, $h_{\\v{\\beta}}(\\v{x}) = \\v{\\beta}_0\\v{x}_0 +\n",
    "    \\v{\\beta}_1\\v{x}_1 + \\ldots + \\v{\\beta}_n\\v{x}_n$. We are trying to find a \n",
    "    $(n+1)$-dimensional vector $\\v{\\beta}$ of parameters that minimise\n",
    "    the loss function $J(\\v{\\beta}) = \\frac{1}{2m}\\sum_{i=1}^m(h_{\\v{\\beta}}(\\v{x}^{(i)}) - \\v{y}^{(i)})^2$. \n",
    "</p>\n",
    "<p>\n",
    "    In the previous lecture, we saw that one approach is to set the gradient of the loss function to zero \n",
    "    and to solve for $\\v{\\beta}$, giving what is called the normal equation. In this lecture, we take a \n",
    "    different approach: we <em>search</em> for the $\\v{\\beta}$ that minimises the loss function.\n",
    "</p>\n",
    "<p>\n",
    "    Conceptually, this approach works as follows:\n",
    "</p>\n",
    "<ul>\n",
    "    <li>\n",
    "         It starts with an initial guess &mdash; e.g. choose each $\\v{\\beta}_j$ randomly or set each to zero\n",
    "    </li>\n",
    "    <li>\n",
    "        It computes $J(\\v{\\beta})$ &mdash; the total error for this hypothesis on the training set\n",
    "    </li>\n",
    "    <li>\n",
    "        And it repeats: it chooses another $\\v{\\beta}$ &mdash; one for which $J(\\v{\\beta})$ is smaller\n",
    "    </li>\n",
    "    <li>\n",
    "        It keeps doing this until $J(\\v{\\beta})$ <b>converges</b> &mdash; changes to $\\v{\\beta}$ do not result\n",
    "        in smaller $J(\\v{\\beta})$\n",
    "    </li>\n",
    "</ul>\n",
    "<p>\n",
    "    So the key to this algorithm is how it comes up with new parameter values, $\\v{\\beta}$.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Gradient Descent</h1>\n",
    "<p>\n",
    "    <b>Gradient descent</b> is a <em>general</em> method for finding parameters $\\v{\\beta}$ that minimize some\n",
    "    function $J_{\\v{\\beta}}$. The idea is to repeatedly make small changes to the values of the parameters that\n",
    "    lead to the greatest immediate decrease in the value of $J(\\v{\\beta})$. The algorithm is as follows: \n",
    "</p>\n",
    "<ul style=\"background: lightgrey; list-style: none\">\n",
    "    <li>\n",
    "        repeat\n",
    "        <ul>\n",
    "            <li>\n",
    "                <em>simultaneously</em> update all $\\v{\\beta}_j$ as follows:<br />\n",
    "                $\\v{\\beta}_j \\gets \\v{\\beta}_j - \\alpha\\frac{\\partial J(\\v{\\beta})}{\\partial\\v{\\beta}_j}$\n",
    "            </li>\n",
    "        </ul>\n",
    "        until convergence\n",
    "     </li>\n",
    "</ul>\n",
    "<p>\n",
    "    $\\alpha$ is called the <b>learning rate</b> and it controls the size of the changes that we make.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>The Importance of Simultaneous Update</h2>\n",
    "<p>\n",
    "    It's important that the updates to the $\\v{\\beta}_j$ are done simultaneously, not one after the other. \n",
    "    We will see how to achieve this using vectorization. The alternative, if you want to stick with a more\n",
    "    conventional programming style, is to use the following statements:\n",
    "</p>\n",
    "<ul style=\"background: lightgrey\">\n",
    "    <li>\n",
    "        $\\v{\\delta}_0 \\gets \\frac{\\partial J(\\v{\\beta})}{\\partial\\v{\\beta}_0}$<br />\n",
    "        $\\v{\\delta}_1 \\gets \\frac{\\partial J(\\v{\\beta})}{\\partial\\v{\\beta}_1}$<br />\n",
    "        $\\vdots$<br />\n",
    "        $\\v{\\delta}_n \\gets \\frac{\\partial J(\\v{\\beta})}{\\partial\\v{\\beta}_n}$<br />\n",
    "        <br />\n",
    "        $\\v{\\beta}_0 \\gets \\v{\\beta}_0 - \\alpha\\v{\\delta}_0$<br />\n",
    "        $\\v{\\beta}_1 \\gets \\v{\\beta}_1 - \\alpha\\v{\\delta}_1$<br />\n",
    "        $\\vdots$<br />\n",
    "        $\\v{\\beta}_n \\gets \\v{\\beta}_n - \\alpha\\v{\\delta}_n$<br />\n",
    "    </li>\n",
    "</ul>\n",
    "<p>\n",
    "    A student reads the above, takes no notice, and simply implements the algorithm with the following \n",
    "    <em>sequence</em> of statements:\n",
    "</p>\n",
    "<ul style=\"background: lightgrey\">\n",
    "    <li>\n",
    "        $\\v{\\beta}_0 \\gets \\v{\\beta}_0 - \\alpha\\frac{\\partial J(\\v{\\beta})}{\\partial\\v{\\beta}_0}$<br />\n",
    "        $\\v{\\beta}_1 \\gets \\v{\\beta}_1 - \\alpha\\frac{\\partial J(\\v{\\beta})}{\\partial\\v{\\beta}_1}$<br />\n",
    "        $\\vdots$<br />\n",
    "        $\\v{\\beta}_n \\gets \\v{\\beta}_n - \\alpha\\frac{\\partial J(\\v{\\beta})}{\\partial\\v{\\beta}_n}$<br />\n",
    "    </li>\n",
    "</ul>\n",
    "<p>\n",
    "    What's the difference?\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Further Questions</h2>\n",
    "<p>\n",
    "    Some further questions that we will discuss in the lecture:\n",
    "</p>\n",
    "<ul>\n",
    "    <li>\n",
    "        Why do these update rules, using derivatives, make intuitive sense. (We will explain this with\n",
    "        a diagram in which we plot $J_{\\beta}$ on the vertical axis and just one of the parameters on\n",
    "        the horizontal axis. We will consider separately the case where the gradient is positive and where\n",
    "        it is negative and we will see how the update rules move us towards a minimum.)\n",
    "    </li>\n",
    "    <li>\n",
    "        In practice, how will we define convergence?\n",
    "    </li>\n",
    "    <li>\n",
    "        What is the role of the learning rate, $\\alpha$?\n",
    "        <ul>\n",
    "            <li>\n",
    "                What if the value of $\\alpha$ is 'too small'?\n",
    "            </li>\n",
    "            <li>\n",
    "                What if the value of $\\alpha$ is 'too large'?\n",
    "            </li>\n",
    "            <li>\n",
    "                Some people suggest a variant of the algorithm in which the value of $\\alpha$ is decreased\n",
    "                over time, i.e. its value in later iterations is smaller. Why do they suggest this? And why\n",
    "                isn't it necessary?\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        What happens if $J_{\\v{\\beta}}$ isn't convex?\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Gradient Descent for OLS Linear Regression</h1>\n",
    "<p>\n",
    "    All that we need to apply this general algorithm to our specific scenario is to plug in the partial\n",
    "    derivative of our definition of $J_{\\beta}$. We want this:\n",
    "    $$\\frac{\\partial\\frac{1}{2m}\\sum_{i=1}^m(h_{\\v{\\beta}}(\\v{x}^{(i)}) - \\v{y}^{(i)})^2}{\\partial\\v{\\beta}_j}$$\n",
    "    which is\n",
    "    $$\\frac{1}{m}\\sum_{i=1}^m(h_{\\v{\\beta}}(\\v{x}^{(i)}) - \\v{y}^{(i)}) \\times \\v{x}_j^{(i)}$$\n",
    "</p>\n",
    "<p>\n",
    "    So we now get gradient descent for OLS linear regression:\n",
    "</p>\n",
    "<ul style=\"background: lightgrey; list-style: none\">\n",
    "    <li>\n",
    "        repeat\n",
    "        <ul>\n",
    "            <li>\n",
    "                <em>simultaneously</em> update $\\v{\\beta}_j$  as follows:<br />\n",
    "                $\\v{\\beta}_j \\gets \\v{\\beta}_j - \n",
    "                    \\alpha\\frac{1}{m}\\sum_{i=1}^m(h_{\\v{\\beta}}(\\v{x}^{(i)}) - \\v{y}^{(i)}) \\times \\v{x}_j^{(i)}$\n",
    "            </li>\n",
    "        </ul>\n",
    "        until convergence\n",
    "     </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Gradient Descent for OLS Linear Regression</h2>\n",
    "<p>\n",
    "    Watch out for the vectorized implementation of the simultaneous update rules!\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def J(beta, X, y):\n",
    "    \"\"\"\n",
    "    Loss function for OLS regression\n",
    "    \"\"\"\n",
    "    h = X.dot(beta)\n",
    "    differences = h.flatten() - y\n",
    "    sq_differences = differences ** 2\n",
    "    return (1.0 / (2 * y.size)) * sq_differences.sum()\n",
    "\n",
    "def gds_for_ols_linear_regression(X, y, alpha, num_iterations):\n",
    "    \"\"\"\n",
    "    Gradient descent search for OLS linear regression.\n",
    "    alpha is the learning rate.\n",
    "    num_iterations is the numer of updates - instead of a better definition of convergence.\n",
    "    It returns parameters beta and also a numpy array of size num_iterations, containing\n",
    "    the value of the loss function, J, after each iteration - so you can plot it.\n",
    "    \"\"\"\n",
    "\n",
    "    Jvals = np.zeros(num_iterations)\n",
    "    m, n = X.shape\n",
    "    beta = np.zeros(n)\n",
    "    \n",
    "    for iter in range(num_iterations):\n",
    "        beta -= (1.0 * alpha / m) * (X.dot(beta) - y).dot(X)\n",
    "        Jvals[iter] = J(beta, X, y)\n",
    " \n",
    "    return beta, Jvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Running the Code</h2>\n",
    "<p>\n",
    "    We'll run it on the Cork property dataset but, for reasons that we will discuss in a later lecture, I'm\n",
    "    going to leave out the floor area feature.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-57.66816632,  49.06247762,  91.69181718])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use pandas to read the CSV file\n",
    "df = pd.read_csv(\"dataset-corkA.csv\")\n",
    "\n",
    "# Insert the extra feature (all ones)\n",
    "df.insert(loc=0, column='ones', value=1)\n",
    "\n",
    "# Get the feature-values and the target values into separate numpy arrays of numbers\n",
    "X = df[['ones', 'bdrms', 'bthrms']].values\n",
    "y = df['price'].values\n",
    "\n",
    "# Run the GDS\n",
    "beta, Jvals = gds_for_ols_linear_regression(X, y, alpha = 0.03, num_iterations = 4000)\n",
    "\n",
    "# Display beta\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    So, using just those two features ($\\v{x}_2$ and $\\v{x}_3$, the number of bedrooms and bathrooms resp.)\n",
    "    and a learning rate of 0.03, after 4000 iterations, the model we have learned is\n",
    "    $$y = -57.67 + 49.06\\v{x_2} + 91.69\\v{x_3}$$\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Sanity Check</h3>\n",
    "<p>\n",
    "    It's a good idea to plot the values of the loss function against the number of iterations. If its value\n",
    "    ever increases, then\n",
    "</p>\n",
    "<ul>\n",
    "    <li>\n",
    "        the code might be incorrect (I think it's OK!)\n",
    "    </li>\n",
    "    <li>\n",
    "        the value of $\\alpha$ is too big and is causing divergence\n",
    "    </li>\n",
    "</ul>\n",
    "<p>\n",
    "    So let's do that:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtgAAAIkCAYAAAAkg1xnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb61f4fc828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.title(\"$J$ during learning\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.xlim(1, Jvals.size)\n",
    "plt.ylabel(\"$J$\")\n",
    "plt.ylim(56000, 70000)\n",
    "xvals = np.linspace(1, Jvals.size, Jvals.size)\n",
    "plt.scatter(xvals, Jvals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Stochastic Gradient Descent</h1>\n",
    "<p>\n",
    "    The algorithm we have presented is sometimes called <b>batch gradient descent</b>. On every iteration,\n",
    "    for every parameter, it computes the loss function over <em>all</em> the examples in the training set.\n",
    "    Even with a highly optimized vectorized implementation, this may not scale well to very, very large \n",
    "    training sets. If there are hundreds of millions of examples, say, then they may not even fit in main\n",
    "    memory, in which case they have to be repeatedly read in from disk.\n",
    "</p>\n",
    "<p>\n",
    "    An alternative is <b>stochastic gradient descent</b> (or 'incremental gradient descent'). It looks at\n",
    "    each example in turn, and modifies the parameters $\\v{\\beta}$ on the basis of that individual example.\n",
    "    Here's the pseudocode:\n",
    "</p>\n",
    "<ul style=\"background: lightgrey; list-style: none\">\n",
    "    <li>\n",
    "        repeat\n",
    "        <ul>\n",
    "            <li>for $i \\gets 1$ to $m$\n",
    "                <ul>\n",
    "                    <li>\n",
    "                        <em>simultaneously</em> update $\\v{\\beta}_j$  as follows:<br />\n",
    "                        $\\v{\\beta}_j \\gets \\v{\\beta}_j - \n",
    "                        \\alpha(h_{\\v{\\beta}}(\\v{x}^{(i)}) - \\v{y}^{(i)}) \\times \\v{x}_j^{(i)}$\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "        until convergence\n",
    "     </li>\n",
    "</ul>\n",
    "<p>\n",
    "    Because it is not guided by the <em>global</em> minimum, some of the individual parameter updates taken in\n",
    "    stochastic gradient descent may not reduce the loss function: its 'journey' towards convergence may be\n",
    "    less direct. On the other hand, if you use it only when you have massive training sets, stochastic\n",
    "    gradient descent might actually take fewer iterations (of the outermost loop) than you require for\n",
    "    typical uses of batch gradient descent.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>OLS Linear Regression: Normal Equation vs Gradient Descent</h1>\n",
    "<p>\n",
    "    Gradient Descent is such an important algorithm in machine learning and AI in general, that it was worth\n",
    "    studying it. Numerous machine learning algorithms make use of it.\n",
    "</p>\n",
    "<p>\n",
    "    But, for OLS linear regression, you probably wouldn't use it. The normal equation gives a much more direct\n",
    "    method. Here's a comparison of the two:\n",
    "</p>\n",
    "<ul>\n",
    "    <li>\n",
    "        Gradient Descent requires that we choose a learning rate, $\\alpha$; \n",
    "        the normal equation does not.\n",
    "    </li>\n",
    "    <li>\n",
    "        Gradient Descent requires a definition of convergence; the normal equation does not.\n",
    "    </li>\n",
    "    <li>\n",
    "        Gradient Descent can have problems with convergence when different features have very different\n",
    "        ranges of values: see the discussion of feature scaling in an upcoming lecture. These problems do not \n",
    "        arise with the normal equation.\n",
    "    </li>\n",
    "    <li>\n",
    "        But Gradient Descent can handle large numbers of features; you might prefer it if the number of \n",
    "        features $n$ is very large (several thousand, say). The normal equation does not scale so well to\n",
    "        very large numbers of features: $(\\v{X}^T\\v{X})$ is $n \\times n$ (where $n$ is the number of features)\n",
    "        and the typical algorithms for computing matrix inverses (or pseudo-inverses) are cubic in $n$ \n",
    "        (and even the algorithms with better complexity are all more than quadratic). Inverting a matrix with \n",
    "        tens of thousands of features can be too slow.\n",
    "    </li>\n",
    "    <li>\n",
    "        Gradient Descent is a general method for minimizing a  loss function: it crops up across a wide\n",
    "        range of learning algorithms. (This is why we studied it!) By contrast, the normal equation is \n",
    "        specific to linear regression.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>There's No Need To Roll Your Own!</h2>\n",
    "<p>\n",
    "    Let's make one final point by way of conclusion: of course, you don't need to implement either of these linear\n",
    "    regression methods yourself!\n",
    "    A good library such as Python's scikit-learn algorithm will give you a method for doing it. You've\n",
    "    already seen it done. But here it is again (and I'll use the same two features as above, for comparison):\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept:  -57.6684429697\n",
      "Coefficients:  [ 49.06255205  91.69181314]\n"
     ]
    }
   ],
   "source": [
    "# Use pandas to read the CSV file\n",
    "df = pd.read_csv(\"dataset-corkA.csv\")\n",
    "\n",
    "# Get the feature-values and the target values into separate numpy arrays of numbers\n",
    "X = df[['bdrms', 'bthrms']].values\n",
    "y = df['price'].values\n",
    "\n",
    "# Create linear regression object\n",
    "estimator = LinearRegression()\n",
    "\n",
    "# Train the model using the data\n",
    "estimator.fit(X, y)\n",
    "\n",
    "# Print the parameters that it learns\n",
    "print('Intercept: ', estimator.intercept_)\n",
    "print('Coefficients: ', estimator.coef_)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
